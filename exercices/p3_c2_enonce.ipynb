{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le squelette de service tel qu'il était présenté dans le screencast du cours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bentoml.service()\n",
    "class TransactionPrediction:\n",
    "    \n",
    "    transaction_classifiers = bentoml.models.get(\n",
    "        \"transaction_below_market_identifier:latest\"\n",
    "    ) \n",
    "    \n",
    "    transaction_regressors = bentoml.models.get(\n",
    "        \"transaction_value_estimator:latest\"\n",
    "    ) \n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.classifier = bentoml.sklearn.load_model(self.transaction_classifier)\n",
    "        self.regressor = bentoml.sklearn.load_model(self.transaction_regressor)\n",
    "\n",
    "    @bentoml.api\n",
    "    def predict(self, transactions: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        \n",
    "        predicted_classes = self.classifier.predict(transactions)\n",
    "        predicted_transactions = self.regressor.predict(transactions)\n",
    "        return (predicted_transactions, predicted_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux approches (entre autres) sont possibles : \n",
    "* Dès que le service sera lancé (avec la commande bentoml serve), tous les modèles (5 de classification et 5 de régression) seront d'abord chargés. C'est ce que nous avons fait ici. \n",
    "* Nous ne chargeons que les 2 modèles pertinents, au moment où un utilisateur appelle l'API pour l'inférence\n",
    "\n",
    "Il n'y a pas de bons choix dans l'absolu, tout dépend des contraintes de projet (en termes de vitesse de réponse souhaitée de l'API, du volume du trafic que l'API va encaisser, etc.). \n",
    "\n",
    "Ici, nous avons procédé avec la 1ere approche, car les modèles sont relativement légers et que nous avons peu de régions.\n",
    "\n",
    "Une approche plus scalable serait d'avoir plusieurs instances actives de la même API, chacune servant une région "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu la définition du endpoint, il est attendu à ce que nous fournissions comme input une région et un ensemble d'observations sous la forme d'un numpy array. Nous pouvons, comme vu dans le cours, \n",
    "* Utiliser le terminal avec curl \n",
    "* Utiliser le Swagger UI \n",
    "* Utiliser Python, avec la librairie requests directement ou indirectement via BentoMl (comme fait ci-dessous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Après avoir lancé la commande bentoml serve, nous pouvons intéragir avec\n",
    "le modèle en précisant la région et les transactions à prédire.\n",
    "'''\n",
    "\n",
    "with bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n",
    "    result = client.predict(region = \"Occitanie\", transactions=test_values)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
