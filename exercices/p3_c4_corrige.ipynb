{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from settings import (\n",
    "    random_state,\n",
    "    PROJECT_PATH,\n",
    "    REGRESSION_TARGET,\n",
    "    CLASSIFICATION_TARGET,\n",
    ")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import mlflow\n",
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce corrigé se base sur la version suivante de Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(\n",
    "    X: pl.DataFrame,\n",
    "    y: pl.Series,\n",
    "    model,\n",
    "    cross_val_type,\n",
    "    scoring_metrics: tuple,\n",
    "    groups=None,\n",
    "):\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X.to_numpy(),\n",
    "        y.to_numpy(),\n",
    "        cv=cross_val_type,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        scoring=scoring_metrics,\n",
    "        groups=groups,\n",
    "    )\n",
    "\n",
    "    scores_dict = {}\n",
    "    for metric in scoring_metrics:\n",
    "        scores_dict[\"average_train_\" + metric] = np.mean(scores[\"train_\" + metric])\n",
    "        scores_dict[\"train_\" + metric + \"_std\"] = np.std(scores[\"train_\" + metric])\n",
    "        scores_dict[\"average_test_\" + metric] = np.mean(scores[\"test_\" + metric])\n",
    "        scores_dict[\"test_\" + metric + \"_std\"] = np.std(scores[\"test_\" + metric])\n",
    "\n",
    "    model.fit(X.to_numpy(), y.to_numpy())\n",
    "\n",
    "    return scores, scores_dict, model\n",
    "\n",
    "def get_features_most_importance(importances, feature_names, threshold=0.8):\n",
    "    sorted_indices = np.argsort(importances)\n",
    "    sorted_importances = importances[sorted_indices][::-1]\n",
    "    sorted_feature_names = [feature_names[i] for i in sorted_indices][::-1]\n",
    "\n",
    "    cumulated_importance = 0\n",
    "    important_features = []\n",
    "\n",
    "    for importance, feature_name in zip(sorted_importances, sorted_feature_names):\n",
    "        cumulated_importance += importance\n",
    "        important_features.append(feature_name)\n",
    "\n",
    "        if cumulated_importance >= threshold:\n",
    "            break\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pl.read_parquet(\n",
    "    os.path.join(PROJECT_PATH, \"transactions_post_feature_engineering.parquet\")\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"../features_used.json\", \"r\") as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "with open(\"../categorical_features_used.json\", \"r\") as f:\n",
    "    categorical_features = json.load(f)\n",
    "\n",
    "numerical_features = [col for col in feature_names if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_v1 = transactions.filter(pl.col(\"annee_transaction\") < 2020)\n",
    "\n",
    "transactions_v2 = transactions.filter(\n",
    "    pl.col(\"annee_transaction\").is_between(2020, 2021)\n",
    ")\n",
    "features_1 = [\n",
    "    \"type_batiment_Appartement\",\n",
    "    \"surface_habitable\",\n",
    "    \"prix_m2_moyen_mois_precedent\",\n",
    "    \"nb_transactions_mois_precedent\",\n",
    "    \"taux_interet\",\n",
    "    \"variation_taux_interet\",\n",
    "    \"acceleration_taux_interet\",\n",
    "]\n",
    "\n",
    "features_2 = features_1 + [\"longitude\", \"latitude\", \"vefa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Attention : </b> Assurez-vous de lancer ce code dans le même dossier où les modèles pré-covid ont été créés ! En effet, quand vous lancez votre MLflow UI (ainsi qu'un objet MlflowClient) sans arguments supplémentaires, le package va utiliser comme modèle registry ceux du dossier actuel. Plus précisement, Mlflow va créer un dossier mlruns, où seront stockés tous vos modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit alors de : \n",
    "* lancer dans votre Terminal la commande mlflow ui en précisant l'adresse du dossier mlruns comme ceci : mlflow ui --backend-store-uri adresse_de_votre_choix\n",
    "* Réaliser la même opération côté Python avec la commande mlflow.set_tracking_uri(\"adresse_de_votre_choix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous déclarons de nouvelles valeurs de tags pour rester cohérent avec le contexte : Nouveau feature engineering post-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_tags = {\n",
    "    \"region\": \"Nouvelle-Aquitaine\",\n",
    "    \"revision_de_donnees\": \"v2\",\n",
    "    \"date_de_construction\": \"Fin 2021\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous précisons à MLflow que nous allons travailler avec l'experiment Nouvelle-Aquitaine comme vu dans le cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n",
    "nouvelle_acquitaine_experiment = mlflow.set_experiment(\"Nouvelle-Aquitaine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons récupérer l'experiment id en printant l'objet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouvelle_acquitaine_experiment_id = '247887458207936819'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_post_covid_nouvelle_acquitaine = transactions_v2.filter(\n",
    "    pl.col(\"nom_region_Nouvelle-Aquitaine\") == 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir comparer des situations comparables, il faut d'abord charger les modèles entrainés sur la période pré-covid, réaliser une inférence sur la nouvelle donnée et observer les résultats. En effet, cela aurait peu de sens de comparer un nouveau modèle entrainé sur une nouvelle donnée avec un ancien modèle entrainé sur une ancienne donnée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cela, nous utilisons la fonctionnalité search_runs. Nous allons utiliser l'experiment id pour trouver l'URI (adresse de stockage) du modèle avec les meilleures performances sur la période pré-covid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il s'agit d'un DataFrame \n",
    "all_experiments = mlflow.search_runs(search_all_experiments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n",
       "       'end_time', 'metrics.train_precision_std',\n",
       "       'metrics.average_train_recall', 'metrics.average_test_precision',\n",
       "       'metrics.average_test_recall', 'metrics.train_recall_std',\n",
       "       'metrics.average_train_precision', 'metrics.test_precision_std',\n",
       "       'metrics.test_f1_std', 'metrics.average_train_f1',\n",
       "       'metrics.average_test_f1', 'metrics.train_f1_std',\n",
       "       'metrics.test_recall_std', 'params.features', 'params.random_state',\n",
       "       'tags.mlflow.user', 'tags.mlflow.source.name',\n",
       "       'tags.mlflow.log-model.history', 'tags.mlflow.runName',\n",
       "       'tags.mlflow.source.type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_experiments.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer ici, nous nous contentons d'une approche simple où l'on choisit le modèle avec le meilleure f1_score moyen. Une approche plus robuste consisterait à comparer les scores en train et test pour vérifier la part d'overfit et regarder l'ecart-type des scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_f1_score = all_experiments[\"metrics.average_test_f1\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precovid_model = all_experiments.loc[\n",
    "    (all_experiments[\"experiment_id\"] == nouvelle_acquitaine_experiment_id) \n",
    "    & (all_experiments[\"metrics.average_test_f1\"] == highest_f1_score)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous recupérons ensuite le Run ID du modèle, à partir duquel nous pouvons déduire son adresse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precovid_model_run_id = best_precovid_model[\"run_id\"].values[0] # En cas de plusieurs modèles avec la même performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande d'identifier le nom du fichier du modèle. C'est le même nom que celui qui a été stocké par la méthode log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileInfo: file_size=None, is_dir=True, path='catboost_classifier'>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_artifacts(best_precovid_model_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = mlflow.artifacts.download_artifacts(\n",
    "  run_id=best_precovid_model_run_id, artifact_path=\"catboost_classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_precovid_model = mlflow.sklearn.load_model(model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons alors un Run qui va réaliser une inférence du modèle pre-covid sur la donnée post-covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/28 17:53:46 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "c:\\Users\\Zakaria\\Desktop\\Data Science\\Openclassrooms\\supervisedlearning\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:406: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with mlflow.start_run(run_name=\"pre_covid_catboost_Nouvelle-Aquitaine_post_covid_data\") as run:\n",
    "\n",
    "    X = transactions_post_covid_nouvelle_acquitaine.drop(\n",
    "        [REGRESSION_TARGET, CLASSIFICATION_TARGET]\n",
    "    ).to_pandas()\n",
    "    y_classification = transactions_post_covid_nouvelle_acquitaine[CLASSIFICATION_TARGET].to_pandas()\n",
    "\n",
    "    classification_scoring_metrics = [\"recall\", \"precision\", \"f1\"]\n",
    "\n",
    "    predictions = best_precovid_model.predict(Pool(X[features_1],y_classification))\n",
    "\n",
    "    scores_dict = {}\n",
    "    scores_dict[\"recall\"] = recall_score(y_classification, predictions)\n",
    "    scores_dict[\"precision\"] = precision_score(y_classification, predictions)\n",
    "    scores_dict[\"f1\"] = f1_score(y_classification, predictions)\n",
    "\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"features\", features_1)\n",
    "\n",
    "    for metric, value in scores_dict.items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "\n",
    "    mlflow.sklearn.log_model(best_precovid_model, \"best_pre_covid_model_post_covid_data\")\n",
    "\n",
    "    dataset_abstraction = mlflow.data.from_pandas(\n",
    "        transactions_post_covid_nouvelle_acquitaine.to_pandas()\n",
    "    )\n",
    "    mlflow.log_input(dataset_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = mlflow.search_runs(\n",
    "    filter_string=\"\"\"\n",
    "    tags.mlflow.runName = 'pre_covid_catboost_Nouvelle-Aquitaine_post_covid_data' \n",
    "    AND status = 'FINISHED'\n",
    "    \"\"\"\n",
    ")\n",
    "run_metrics = run_metrics[[col for col in run_metrics.columns if col.startswith(\"metrics.\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics.f1</th>\n",
       "      <th>metrics.precision</th>\n",
       "      <th>metrics.recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515822</td>\n",
       "      <td>0.650209</td>\n",
       "      <td>0.427471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metrics.f1  metrics.precision  metrics.recall\n",
       "0    0.515822           0.650209        0.427471"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous réalisons ensuite notre Run de la même manière que dans le cours, en utilisant le nouveau jeu de features et de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/28 18:11:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "c:\\Users\\Zakaria\\Desktop\\Data Science\\Openclassrooms\\supervisedlearning\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:406: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with mlflow.start_run(run_name=\"catboost_Nouvelle-Aquitaine_post_covid\") as run:\n",
    "\n",
    "    X = transactions_post_covid_nouvelle_acquitaine.drop(\n",
    "        [REGRESSION_TARGET, CLASSIFICATION_TARGET]\n",
    "    ).to_pandas()\n",
    "    y_classification = transactions_post_covid_nouvelle_acquitaine[CLASSIFICATION_TARGET].to_pandas()\n",
    "\n",
    "    catboost_model = CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    classification_scoring_metrics = [\"recall\", \"precision\", \"f1\"]\n",
    "\n",
    "    scores, scores_dict, catboost_model = perform_cross_validation(\n",
    "        X=X[features_2],\n",
    "        y=y_classification,\n",
    "        model=catboost_model,\n",
    "        cross_val_type=StratifiedKFold(),\n",
    "        scoring_metrics=classification_scoring_metrics,\n",
    "    )\n",
    "\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"features\", features_2)\n",
    "\n",
    "    for metric, value in scores_dict.items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "\n",
    "    mlflow.sklearn.log_model(catboost_model, \"catboost_classifier_post_covid\")\n",
    "\n",
    "    dataset_abstraction = mlflow.data.from_pandas(\n",
    "        transactions_post_covid_nouvelle_acquitaine.to_pandas()\n",
    "    )\n",
    "    mlflow.log_input(dataset_abstraction)\n",
    "\n",
    "feature_importances = catboost_model.get_feature_importance(Pool(X[features_2]))\n",
    "most_important_features = get_features_most_importance(\n",
    "    feature_importances, features_2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu d'utiliser le MLflow UI, nous allons programmatiquement chercher le meilleur modèle parmi ceux post covid et ceux pré-covid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_new_model = mlflow.search_runs(\n",
    "    filter_string=\"\"\"\n",
    "    tags.mlflow.runName = 'catboost_Nouvelle-Aquitaine_post_covid'\n",
    "    AND status = 'FINISHED'\n",
    "    \"\"\"\n",
    ")\n",
    "run_metrics_new_model = run_metrics_new_model[[col for col in run_metrics_new_model.columns if col.startswith(\"metrics.\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics.train_precision_std</th>\n",
       "      <th>metrics.average_train_recall</th>\n",
       "      <th>metrics.average_test_precision</th>\n",
       "      <th>metrics.average_test_recall</th>\n",
       "      <th>metrics.train_recall_std</th>\n",
       "      <th>metrics.average_train_precision</th>\n",
       "      <th>metrics.test_precision_std</th>\n",
       "      <th>metrics.test_f1_std</th>\n",
       "      <th>metrics.average_train_f1</th>\n",
       "      <th>metrics.average_test_f1</th>\n",
       "      <th>metrics.train_f1_std</th>\n",
       "      <th>metrics.test_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.552081</td>\n",
       "      <td>0.436451</td>\n",
       "      <td>0.465252</td>\n",
       "      <td>0.01664</td>\n",
       "      <td>0.819325</td>\n",
       "      <td>0.101887</td>\n",
       "      <td>0.10322</td>\n",
       "      <td>0.659542</td>\n",
       "      <td>0.442434</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>0.138184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   metrics.train_precision_std  metrics.average_train_recall  \\\n",
       "0                     0.006768                      0.552081   \n",
       "\n",
       "   metrics.average_test_precision  metrics.average_test_recall  \\\n",
       "0                        0.436451                     0.465252   \n",
       "\n",
       "   metrics.train_recall_std  metrics.average_train_precision  \\\n",
       "0                   0.01664                         0.819325   \n",
       "\n",
       "   metrics.test_precision_std  metrics.test_f1_std  metrics.average_train_f1  \\\n",
       "0                    0.101887              0.10322                  0.659542   \n",
       "\n",
       "   metrics.average_test_f1  metrics.train_f1_std  metrics.test_recall_std  \n",
       "0                 0.442434              0.012896                 0.138184  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics_new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
